{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae1d236-5c7f-492c-976b-c5069e2dfa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standard libraries\n",
    "import collections\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(\"..\"))  # Parent directory of 'soil_id'\n",
    "\n",
    "# Now import modules as a package\n",
    "from soil_id import config\n",
    "from soil_id.db import (\n",
    "    get_WISE30sec_data,\n",
    "    get_WRB_descriptions,\n",
    "    getSG_descriptions,\n",
    "    load_model_output,\n",
    "    save_model_output,\n",
    "    save_rank_output,\n",
    "    save_soilgrids_output,\n",
    ")\n",
    "from soil_id.services import (\n",
    "    get_soilgrids_classification_data,\n",
    "    get_soilgrids_property_data,\n",
    ")\n",
    "from soil_id.utils import (\n",
    "    agg_data_layer,\n",
    "    assign_max_distance_scores,\n",
    "    calculate_location_score,\n",
    "    compute_data_completeness,\n",
    "    drop_cokey_horz,\n",
    "    extract_values,\n",
    "    extract_WISE_data,\n",
    "    getCF_fromClass,\n",
    "    getClay,\n",
    "    getProfile,\n",
    "    getSand,\n",
    "    getTexture,\n",
    "    gower_distances,\n",
    "    pedon_color,\n",
    "    sg_get_and_agg,\n",
    "    silt_calc,\n",
    "    max_comp_depth,\n",
    "    convert_geometry_to_utm,\n",
    "    calculate_distances_and_intersections,\n",
    ")\n",
    "\n",
    "from soil_id.color import(\n",
    "    calculate_deltaE2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e82174a-108d-4493-9cc3-1243975c574e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef calculate_distances_and_intersections(mu_geo, point):\\n    \"\"\"\\n    Calculate distances and intersections of geometries to a point.\\n\\n    Args:\\n        mu_geo (GeoDataFrame): GeoDataFrame containing mapunit geometries.\\n        point (Point): Shapely Point object for the location of interest.\\n\\n    Returns:\\n        DataFrame: Contains mapunit keys, distances, and intersection flags.\\n    \"\"\"\\n\\n    # Ensure the point is wrapped in a GeoDataFrame and projected correctly\\n    point_utm, epsg_code = convert_geometry_to_utm(point)\\n    \\n    # Ensure the point is a GeoDataFrame (for compatibility)\\n    if not isinstance(point_utm, gpd.GeoDataFrame):\\n        point_utm = gpd.GeoDataFrame(geometry=[point_utm], crs=epsg_code)\\n    \\n    # Transform the GeoDataFrame to the same UTM CRS\\n    mu_geo_utm = mu_geo.to_crs(epsg_code)\\n    \\n    # Reset index for clean operations\\n    mu_geo_utm = mu_geo_utm.reset_index(drop=True)\\n    point_utm = point_utm.reset_index(drop=True)\\n    \\n    # Extract the single geometry for the point\\n    point_geometry = point_utm.geometry.iloc[0]\\n    \\n    # Calculate distances and intersections\\n    distances = mu_geo_utm[\"geometry\"].distance(point_geometry)\\n    intersects = mu_geo_utm[\"geometry\"].intersects(point_geometry)\\n    return pd.DataFrame(\\n        {\"MUGLB_NEW\": mu_geo_utm[\"MUGLB_NEW\"], \"dist_meters\": distances, \"pt_intersect\": intersects}\\n    )\\n\\n\\ndef convert_geometry_to_utm(geometry, src_crs=\"epsg:4326\", target_crs=None):\\n    \"\"\"\\n    Transforms a given geometry from its source coordinate reference system (CRS)\\n    to an appropriate Universal Transverse Mercator (UTM) CRS based on the geometry\\'s\\n    centroid location.\\n\\n    Parameters:\\n    - geometry (shapely.geometry.base.BaseGeometry): The geometry to transform, which\\n      could be any type of geometry, e.g., Point, Polygon.\\n    - src_crs (str, optional): The EPSG code of the source CRS of the geometry. Defaults\\n      to \\'epsg:4326\\'.\\n    - target_crs (str, optional): The EPSG code of the target CRS to which the geometry\\n      should be transformed. If None, the appropriate UTM CRS based on the geometry\\'s\\n      centroid will be calculated.\\n\\n    Returns:\\n    - tuple: A tuple containing the transformed geometry and the EPSG code of the\\n      target UTM CRS. The transformed geometry is in the new CRS, and distances from\\n      this geometry can be accurately calculated in linear units (meters).\\n\\n    Notes:\\n    - If the target_crs is not provided, the function calculates the correct UTM zone\\n      based on the longitude (from -180 to 180 degrees) and adjusts for the northern or\\n      southern hemisphere based on the latitude.\\n    - This function assumes the geometry is already in the specified source CRS and will\\n      convert it directly to the target CRS without additional CRS transformations.\\n    \"\"\"\\n    # If geometry is not a GeoDataFrame, wrap it into one\\n    if isinstance(geometry, Point):\\n        geometry = gpd.GeoDataFrame(geometry=[geometry], crs=src_crs)\\n    elif isinstance(geometry, gpd.GeoSeries):\\n        geometry = geometry.to_frame(name=\\'geometry\\')\\n\\n    # Project to source CRS (ensure proper handling)\\n    geometry = geometry.to_crs(src_crs)\\n\\n    # Calculate the centroid\\n    centroid = geometry.centroid.iloc[0]\\n    lon, lat = centroid.x, centroid.y\\n\\n    # Determine the UTM zone dynamically\\n    utm_zone = int((lon + 180) / 6) + 1\\n    hemisphere = \"north\" if lat >= 0 else \"south\"\\n    epsg_code = f\"326{utm_zone:02d}\" if hemisphere == \"north\" else f\"327{utm_zone:02d}\"\\n    target_crs = f\"EPSG:{epsg_code}\"\\n\\n    # Reproject geometry to the UTM CRS\\n    geometry_utm = geometry.to_crs(target_crs)\\n\\n    return geometry_utm, target_crs\\n\\n\\n\\ndef get_WISE30sec_data_csv(csv_file_path, MUGLB_NEW_Select):\\n    \"\"\"\\n    Retrieve WISE 30-second data from a CSV file based on selected MUGLB_NEW values.\\n\\n    Parameters:\\n    - csv_file_path (str): Path to the CSV file.\\n    - MUGLB_NEW_Select (list): List of MUGLB_NEW values to filter.\\n\\n    Returns:\\n    - pd.DataFrame: Filtered data.\\n    \"\"\"\\n    try:\\n        # Read the CSV file into a DataFrame\\n        data = pd.read_csv(csv_file_path)\\n        \\n        # Filter the DataFrame based on the MUGLB_NEW values\\n        filtered_data = data[data[\"MUGLB_NEW\"].isin(MUGLB_NEW_Select)]\\n        \\n        # Select specific columns\\n        selected_columns = [\\n            \"MUGLB_NEW\", \"COMPID\", \"id\", \"MU_GLOBAL\", \"NEWSUID\", \"SCID\", \"PROP\", \"CLAF\",\\n            \"PRID\", \"Layer\", \"TopDep\", \"BotDep\", \"CFRAG\", \"SDTO\", \"STPC\", \"CLPC\",\\n            \"CECS\", \"PHAQ\", \"ELCO\", \"SU_name\", \"FAO_SYS\"\\n        ]\\n        filtered_data = filtered_data[selected_columns]\\n        \\n        return filtered_data\\n\\n    except FileNotFoundError as e:\\n        logging.error(f\"File not found: {e}\")\\n        return None\\n    except Exception as err:\\n        logging.error(f\"An error occurred: {err}\")\\n        return None\\n\\n\\ndef get_WRB_descriptions_from_txt(WRB_Comp_List, txt_file_path):\\n    \"\"\"\\n    Retrieve WRB descriptions from a text file based on the provided WRB component list.\\n\\n    Parameters:\\n    - WRB_Comp_List (list): List of WRB components to filter.\\n    - txt_file_path (str): Path to the text file containing WRB descriptions.\\n\\n    Returns:\\n    - pd.DataFrame: Filtered DataFrame with WRB descriptions.\\n    \"\"\"\\n    try:\\n        # Load the text file using pandas\\n        data = pd.read_csv(txt_file_path, delimiter=\"\\t\")\\n        \\n        # Ensure the column names match the expected format\\n        required_columns = [\\n            \"WRB_tax\",\\n            \"WRB_tax_en\",\\n            \"Description_en\",\\n            \"Management_en\",\\n            \"WRB_tax_es\",\\n            \"Description_es\",\\n            \"Management_es\",\\n            \"WRB_tax_ks\",\\n            \"Description_ks\",\\n            \"Management_ks\",\\n            \"WRB_tax_fr\",\\n            \"Description_fr\",\\n            \"Management_fr\",\\n        ]\\n        if not all(col in data.columns for col in required_columns):\\n            raise ValueError(\"Text file is missing required columns.\")\\n        \\n        # Filter the data to include only rows with WRB_tax in WRB_Comp_List\\n        filtered_data = data[data[\"WRB_tax\"].isin(WRB_Comp_List)]\\n        \\n        return filtered_data\\n    except Exception as err:\\n        logging.error(f\"Error while retrieving WRB descriptions: {err}\")\\n        return None\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# global\n",
    "# Standard libraries\n",
    "import logging\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Third-party libraries\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from numpy.linalg import cholesky\n",
    "from osgeo import ogr\n",
    "from rosetta import SoilData, rosetta\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.sparse import issparse\n",
    "from scipy.stats import entropy, norm\n",
    "from shapely.geometry import Point, box\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.utils import validation\n",
    "\n",
    "\n",
    "def calculate_distances_and_intersections(mu_geo, point):\n",
    "    \"\"\"\n",
    "    Calculate distances and intersections of geometries to a point.\n",
    "\n",
    "    Args:\n",
    "        mu_geo (GeoDataFrame): GeoDataFrame containing mapunit geometries.\n",
    "        point (Point): Shapely Point object for the location of interest.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Contains mapunit keys, distances, and intersection flags.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the point is wrapped in a GeoDataFrame and projected correctly\n",
    "    point_utm, epsg_code = convert_geometry_to_utm(point)\n",
    "    \n",
    "    # Ensure the point is a GeoDataFrame (for compatibility)\n",
    "    if not isinstance(point_utm, gpd.GeoDataFrame):\n",
    "        point_utm = gpd.GeoDataFrame(geometry=[point_utm], crs=epsg_code)\n",
    "    \n",
    "    # Transform the GeoDataFrame to the same UTM CRS\n",
    "    mu_geo_utm = mu_geo.to_crs(epsg_code)\n",
    "    \n",
    "    # Reset index for clean operations\n",
    "    mu_geo_utm = mu_geo_utm.reset_index(drop=True)\n",
    "    point_utm = point_utm.reset_index(drop=True)\n",
    "    \n",
    "    # Extract the single geometry for the point\n",
    "    point_geometry = point_utm.geometry.iloc[0]\n",
    "    \n",
    "    # Calculate distances and intersections\n",
    "    distances = mu_geo_utm[\"geometry\"].distance(point_geometry)\n",
    "    intersects = mu_geo_utm[\"geometry\"].intersects(point_geometry)\n",
    "    return pd.DataFrame(\n",
    "        {\"MUGLB_NEW\": mu_geo_utm[\"MUGLB_NEW\"], \"dist_meters\": distances, \"pt_intersect\": intersects}\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_geometry_to_utm(geometry, src_crs=\"epsg:4326\", target_crs=None):\n",
    "    \"\"\"\n",
    "    Transforms a given geometry from its source coordinate reference system (CRS)\n",
    "    to an appropriate Universal Transverse Mercator (UTM) CRS based on the geometry's\n",
    "    centroid location.\n",
    "\n",
    "    Parameters:\n",
    "    - geometry (shapely.geometry.base.BaseGeometry): The geometry to transform, which\n",
    "      could be any type of geometry, e.g., Point, Polygon.\n",
    "    - src_crs (str, optional): The EPSG code of the source CRS of the geometry. Defaults\n",
    "      to 'epsg:4326'.\n",
    "    - target_crs (str, optional): The EPSG code of the target CRS to which the geometry\n",
    "      should be transformed. If None, the appropriate UTM CRS based on the geometry's\n",
    "      centroid will be calculated.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the transformed geometry and the EPSG code of the\n",
    "      target UTM CRS. The transformed geometry is in the new CRS, and distances from\n",
    "      this geometry can be accurately calculated in linear units (meters).\n",
    "\n",
    "    Notes:\n",
    "    - If the target_crs is not provided, the function calculates the correct UTM zone\n",
    "      based on the longitude (from -180 to 180 degrees) and adjusts for the northern or\n",
    "      southern hemisphere based on the latitude.\n",
    "    - This function assumes the geometry is already in the specified source CRS and will\n",
    "      convert it directly to the target CRS without additional CRS transformations.\n",
    "    \"\"\"\n",
    "    # If geometry is not a GeoDataFrame, wrap it into one\n",
    "    if isinstance(geometry, Point):\n",
    "        geometry = gpd.GeoDataFrame(geometry=[geometry], crs=src_crs)\n",
    "    elif isinstance(geometry, gpd.GeoSeries):\n",
    "        geometry = geometry.to_frame(name='geometry')\n",
    "\n",
    "    # Project to source CRS (ensure proper handling)\n",
    "    geometry = geometry.to_crs(src_crs)\n",
    "\n",
    "    # Calculate the centroid\n",
    "    centroid = geometry.centroid.iloc[0]\n",
    "    lon, lat = centroid.x, centroid.y\n",
    "\n",
    "    # Determine the UTM zone dynamically\n",
    "    utm_zone = int((lon + 180) / 6) + 1\n",
    "    hemisphere = \"north\" if lat >= 0 else \"south\"\n",
    "    epsg_code = f\"326{utm_zone:02d}\" if hemisphere == \"north\" else f\"327{utm_zone:02d}\"\n",
    "    target_crs = f\"EPSG:{epsg_code}\"\n",
    "\n",
    "    # Reproject geometry to the UTM CRS\n",
    "    geometry_utm = geometry.to_crs(target_crs)\n",
    "\n",
    "    return geometry_utm, target_crs\n",
    "\n",
    "\n",
    "def get_WISE30sec_data(MUGLB_NEW_Select):\n",
    "    \"\"\"\n",
    "    Retrieve WISE 30 second data based on selected MUGLB_NEW values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = get_datastore_connection()\n",
    "        cur = conn.cursor()\n",
    "        placeholders = \", \".join([\"%s\"] * len(MUGLB_NEW_Select))\n",
    "        sql = \"\"\"SELECT MUGLB_NEW, COMPID, id, MU_GLOBAL, NEWSUID, SCID, PROP, CLAF,\n",
    "                       PRID, Layer, TopDep, BotDep,  CFRAG,  SDTO,  STPC,  CLPC, CECS,\n",
    "                       PHAQ, ELCO, SU_name, FAO_SYS\n",
    "                  FROM  wise_soil_data\n",
    "                  WHERE MUGLB_NEW IN (%s)\"\"\"\n",
    "        cur.execute(sql, placeholders)\n",
    "        results = cur.fetchall()\n",
    "        data = pd.DataFrame(\n",
    "            results,\n",
    "            columns=[\n",
    "                \"MUGLB_NEW\",\n",
    "                \"COMPID\",\n",
    "                \"id\",\n",
    "                \"MU_GLOBAL\",\n",
    "                \"NEWSUID\",\n",
    "                \"SCID\",\n",
    "                \"PROP\",\n",
    "                \"CLAF\",\n",
    "                \"PRID\",\n",
    "                \"Layer\",\n",
    "                \"TopDep\",\n",
    "                \"BotDep\",\n",
    "                \"CFRAG\",\n",
    "                \"SDTO\",\n",
    "                \"STPC\",\n",
    "                \"CLPC\",\n",
    "                \"CECS\",\n",
    "                \"PHAQ\",\n",
    "                \"ELCO\",\n",
    "                \"SU_name\",\n",
    "                \"FAO_SYS\",\n",
    "            ],\n",
    "        )\n",
    "        return data\n",
    "    except Exception as err:\n",
    "        logging.error(err)\n",
    "        return None\n",
    "    finally:\n",
    "        conn.close()\n",
    "        \n",
    "def get_WISE30sec_data_csv(csv_file_path, MUGLB_NEW_Select):\n",
    "    \"\"\"\n",
    "    Retrieve WISE 30-second data from a CSV file based on selected MUGLB_NEW values.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_file_path (str): Path to the CSV file.\n",
    "    - MUGLB_NEW_Select (list): List of MUGLB_NEW values to filter.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        data = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # Filter the DataFrame based on the MUGLB_NEW values\n",
    "        filtered_data = data[data[\"MUGLB_NEW\"].isin(MUGLB_NEW_Select)]\n",
    "        \n",
    "        # Select specific columns\n",
    "        selected_columns = [\n",
    "            \"MUGLB_NEW\", \"COMPID\", \"id\", \"MU_GLOBAL\", \"NEWSUID\", \"SCID\", \"PROP\", \"CLAF\",\n",
    "            \"PRID\", \"Layer\", \"TopDep\", \"BotDep\", \"CFRAG\", \"SDTO\", \"STPC\", \"CLPC\",\n",
    "            \"CECS\", \"PHAQ\", \"ELCO\", \"SU_name\", \"FAO_SYS\"\n",
    "        ]\n",
    "        filtered_data = filtered_data[selected_columns]\n",
    "        \n",
    "        return filtered_data\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {e}\")\n",
    "        return None\n",
    "    except Exception as err:\n",
    "        logging.error(f\"An error occurred: {err}\")\n",
    "        return None\n",
    "\n",
    "def extract_WISE_data(lon, lat, file_path, buffer_dist=10000):\n",
    "    # Create LPKS point\n",
    "    point_geo = gpd.GeoDataFrame(\n",
    "        geometry=[Point(lon, lat)],\n",
    "        crs=\"EPSG:4326\"  # Assign CRS\n",
    "    )\n",
    "    \n",
    "    # Get the appropriate UTM CRS for the location\n",
    "    point_utm, epsg_code = convert_geometry_to_utm(point_geo)\n",
    "    print(f\"Using UTM CRS: {epsg_code}\")  # Log the CRS being used\n",
    "    \n",
    "    # Create a buffer in meters and get the bounding box\n",
    "    bounding_box = point_utm.buffer(buffer_dist).envelope\n",
    "    \n",
    "    # Reproject the bounding box back to geographic CRS (WGS84)\n",
    "    bounding_box = bounding_box.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Read and clip the data using the bounding box\n",
    "    hwsd = gpd.read_file(file_path, bbox=bounding_box)\n",
    "\n",
    "    # Filter data to consider unique map units\n",
    "    mu_geo = hwsd[[\"MUGLB_NEW\", \"geometry\"]].drop_duplicates(subset=\"MUGLB_NEW\")\n",
    "    mu_id_dist = calculate_distances_and_intersections(mu_geo, point_geo)\n",
    "    mu_id_dist.loc[mu_id_dist[\"pt_intersect\"], \"dist_meters\"] = 0\n",
    "    mu_id_dist[\"distance\"] = mu_id_dist.groupby(\"MUGLB_NEW\")[\"dist_meters\"].transform(min)\n",
    "    mu_id_dist = mu_id_dist.nsmallest(2, \"distance\")\n",
    "\n",
    "    hwsd = hwsd.drop(columns=[\"geometry\"])\n",
    "    hwsd = pd.merge(mu_id_dist, hwsd, on=\"MUGLB_NEW\", how=\"left\").drop_duplicates()\n",
    "\n",
    "    MUGLB_NEW_Select = hwsd[\"MUGLB_NEW\"].tolist()\n",
    "    # use revised function to extract data from csv instead of database\n",
    "    wise_data = get_WISE30sec_data_csv(csv_file_path='/mnt/c/LandPKS_API_SoilID-master/global/wise_full_soil.csv', MUGLB_NEW_Select= MUGLB_NEW_Select)\n",
    "    wise_data = pd.merge(wise_data, mu_id_dist, on=\"MUGLB_NEW\", how=\"left\")\n",
    "\n",
    "    return wise_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a68164d-20e1-4c7d-9a9c-3f0cc610d073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25371/1466463946.py:98: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = geometry.centroid.iloc[0]\n",
      "/tmp/ipykernel_25371/1466463946.py:98: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = geometry.centroid.iloc[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using UTM CRS: EPSG:32630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25371/1466463946.py:220: FutureWarning: The provided callable <built-in function min> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  mu_id_dist[\"distance\"] = mu_id_dist.groupby(\"MUGLB_NEW\")[\"dist_meters\"].transform(min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    MUGLB_NEW  COMPID    id  MU_GLOBAL     NEWSUID  SCID  PROP CLAF   PRID  \\\n",
      "0        1453  107895  4676       1453  WD10001453     1    80  LXf  LXf/A   \n",
      "1        1453  107895  4677       1453  WD10001453     1    80  LXf  LXf/A   \n",
      "2        1453  107895  4678       1453  WD10001453     1    80  LXf  LXf/A   \n",
      "3        1453  107895  4679       1453  WD10001453     1    80  LXf  LXf/A   \n",
      "4        1453  107895  4680       1453  WD10001453     1    80  LXf  LXf/A   \n",
      "5        1453  107895  4681       1453  WD10001453     1    80  LXf  LXf/A   \n",
      "6        1453  107896  4682       1453  WD10001453     2    10  LPq  LPq/A   \n",
      "7        1453  107897  4683       1453  WD10001453     3    10  CMe  CMe/A   \n",
      "8        1453  107897  4684       1453  WD10001453     3    10  CMe  CMe/A   \n",
      "9        1453  107897  4685       1453  WD10001453     3    10  CMe  CMe/A   \n",
      "10       1453  107897  4686       1453  WD10001453     3    10  CMe  CMe/A   \n",
      "11       1453  107897  4687       1453  WD10001453     3    10  CMe  CMe/A   \n",
      "12       1453  107897  4688       1453  WD10001453     3    10  CMe  CMe/A   \n",
      "13       1530  108119  5549       1530  WD10001530     1    60  LXp  LXp/A   \n",
      "14       1530  108119  5550       1530  WD10001530     1    60  LXp  LXp/A   \n",
      "15       1530  108119  5551       1530  WD10001530     1    60  LXp  LXp/A   \n",
      "16       1530  108119  5552       1530  WD10001530     1    60  LXp  LXp/A   \n",
      "17       1530  108119  5553       1530  WD10001530     1    60  LXp  LXp/A   \n",
      "18       1530  108119  5554       1530  WD10001530     1    60  LXp  LXp/A   \n",
      "19       1530  108120  5555       1530  WD10001530     2    30  LVg  LVg/A   \n",
      "20       1530  108120  5556       1530  WD10001530     2    30  LVg  LVg/A   \n",
      "21       1530  108120  5557       1530  WD10001530     2    30  LVg  LVg/A   \n",
      "22       1530  108120  5558       1530  WD10001530     2    30  LVg  LVg/A   \n",
      "23       1530  108120  5559       1530  WD10001530     2    30  LVg  LVg/A   \n",
      "24       1530  108120  5560       1530  WD10001530     2    30  LVg  LVg/A   \n",
      "25       1530  108121  5561       1530  WD10001530     3    10  LPq  LPq/A   \n",
      "\n",
      "   Layer  ...  STPC  CLPC  CECS  PHAQ  ELCO            SU_name  FAO_SYS  \\\n",
      "0     D1  ...    16    16     8   6.2     0    Ferric Lixisols    FAO90   \n",
      "1     D2  ...    15    23     6   6.1     0    Ferric Lixisols    FAO90   \n",
      "2     D3  ...    14    30     7   5.9     0    Ferric Lixisols    FAO90   \n",
      "3     D4  ...    14    35     7   5.8     0    Ferric Lixisols    FAO90   \n",
      "4     D5  ...    16    36     7   5.8     0    Ferric Lixisols    FAO90   \n",
      "5     D6  ...    17    36     7   5.8     0    Ferric Lixisols    FAO90   \n",
      "6     D1  ...    29    20    16   6.7     1   Lithic Leptosols    FAO90   \n",
      "7     D1  ...    31    29    22   6.4     2   Eutric Cambisols    FAO90   \n",
      "8     D2  ...    29    32    21   6.4     1   Eutric Cambisols    FAO90   \n",
      "9     D3  ...    29    33    22   6.5     1   Eutric Cambisols    FAO90   \n",
      "10    D4  ...    28    32    23   6.7     1   Eutric Cambisols    FAO90   \n",
      "11    D5  ...    28    31    23   6.9     1   Eutric Cambisols    FAO90   \n",
      "12    D6  ...    31    31    25   7.0     1   Eutric Cambisols    FAO90   \n",
      "13    D1  ...    22    14     5   6.2     0  Plinthic Lixisols    FAO90   \n",
      "14    D2  ...    21    19     4   5.9     0  Plinthic Lixisols    FAO90   \n",
      "15    D3  ...    20    26     5   5.8     0  Plinthic Lixisols    FAO90   \n",
      "16    D4  ...    20    28     6   5.8     0  Plinthic Lixisols    FAO90   \n",
      "17    D5  ...    22    28     6   5.8     0  Plinthic Lixisols    FAO90   \n",
      "18    D6  ...    23    28     6   5.8     0  Plinthic Lixisols    FAO90   \n",
      "19    D1  ...    22    16     9   6.3     1    Gleyic Luvisols    FAO90   \n",
      "20    D2  ...    20    23    11   6.3     1    Gleyic Luvisols    FAO90   \n",
      "21    D3  ...    18    33    14   6.5     1    Gleyic Luvisols    FAO90   \n",
      "22    D4  ...    19    35    15   6.7     2    Gleyic Luvisols    FAO90   \n",
      "23    D5  ...    21    36    16   6.9     2    Gleyic Luvisols    FAO90   \n",
      "24    D6  ...    22    36    16   7.0     3    Gleyic Luvisols    FAO90   \n",
      "25    D1  ...    29    20    16   6.7     1   Lithic Leptosols    FAO90   \n",
      "\n",
      "     dist_meters  pt_intersect      distance  \n",
      "0       0.000000          True      0.000000  \n",
      "1       0.000000          True      0.000000  \n",
      "2       0.000000          True      0.000000  \n",
      "3       0.000000          True      0.000000  \n",
      "4       0.000000          True      0.000000  \n",
      "5       0.000000          True      0.000000  \n",
      "6       0.000000          True      0.000000  \n",
      "7       0.000000          True      0.000000  \n",
      "8       0.000000          True      0.000000  \n",
      "9       0.000000          True      0.000000  \n",
      "10      0.000000          True      0.000000  \n",
      "11      0.000000          True      0.000000  \n",
      "12      0.000000          True      0.000000  \n",
      "13  10697.101221         False  10697.101221  \n",
      "14  10697.101221         False  10697.101221  \n",
      "15  10697.101221         False  10697.101221  \n",
      "16  10697.101221         False  10697.101221  \n",
      "17  10697.101221         False  10697.101221  \n",
      "18  10697.101221         False  10697.101221  \n",
      "19  10697.101221         False  10697.101221  \n",
      "20  10697.101221         False  10697.101221  \n",
      "21  10697.101221         False  10697.101221  \n",
      "22  10697.101221         False  10697.101221  \n",
      "23  10697.101221         False  10697.101221  \n",
      "24  10697.101221         False  10697.101221  \n",
      "25  10697.101221         False  10697.101221  \n",
      "\n",
      "[26 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "wise_data = extract_WISE_data(lon = -1.57, lat=8.95, file_path = '/mnt/c/LandPKS_API_SoilID-master/global/wise30sec_poly_simp_soil.gpkg', buffer_dist=10000)\n",
    "print(wise_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33fd207d-0b5a-4512-afd7-973d4e2f9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "#                                 getSoilLocationBasedGlobal                                     #\n",
    "##################################################################################################\n",
    "def getSoilLocationBasedGlobal(lon, lat, plot_id):\n",
    "    # Extract HWSD-WISE Data\n",
    "    # Note: Need to convert HWSD shp to gpkg file\n",
    "    wise_data = extract_WISE_data(\n",
    "        lon,\n",
    "        lat,\n",
    "        # Temporarily change file path\n",
    "        file_path = '/mnt/c/LandPKS_API_SoilID-master/global/wise30sec_poly_simp_soil.gpkg',\n",
    "        #file_path=config.WISE_PATH,\n",
    "        #layer_name=None,\n",
    "        buffer_dist=10000,\n",
    "    )\n",
    "\n",
    "    # Component Data\n",
    "    mucompdata_pd = wise_data[[\"MUGLB_NEW\", \"SU_name\", \"distance\", \"PROP\", \"COMPID\", \"FAO_SYS\"]]\n",
    "    mucompdata_pd.columns = [\"mukey\", \"compname\", \"distance\", \"share\", \"cokey\", \"fss\"]\n",
    "    mucompdata_pd[\"distance\"] = pd.to_numeric(mucompdata_pd[\"distance\"])\n",
    "    mucompdata_pd[\"share\"] = pd.to_numeric(mucompdata_pd[\"share\"])\n",
    "    mucompdata_pd = mucompdata_pd.drop_duplicates().reset_index(drop=True)\n",
    " \n",
    "    ##############################################################################################\n",
    "    # Individual probability\n",
    "    # Based on Fan et al 2018 EQ 1, the conditional probability for each component is calculated\n",
    "    # by taking the sum of all occurances of a component in the home and adjacent mapunits and\n",
    "    # dividing this by the sum of all map units and components. We have modified this approach\n",
    "    # so that each instance of a component occurance is evaluated separately and assinged a\n",
    "    # weight and the max distance score for each component group is assigned to all component\n",
    "    # instances.\n",
    "    ##############################################################################################\n",
    "    ExpCoeff = -0.00036888  # Decays to 0.25 @ 10km\n",
    "    loc_scores = []\n",
    "    mucompdata_grouped = mucompdata_pd.groupby([\"mukey\", \"cokey\"], sort=False)\n",
    "\n",
    "    for (mukey, cokey), group in mucompdata_grouped:\n",
    "        loc_score = calculate_location_score(group, ExpCoeff)\n",
    "        loc_scores.append({\"cokey\": cokey, \"mukey\": mukey, \"distance_score\": round(loc_score, 3)})\n",
    "\n",
    "    loc_top_pd = pd.DataFrame(loc_scores)\n",
    "    loc_top_comp_prob = loc_top_pd.groupby(\"cokey\").distance_score.sum()\n",
    "    loc_bot_prob_sum = loc_top_pd.distance_score.sum()\n",
    "    cond_prob = (loc_top_comp_prob / loc_bot_prob_sum).reset_index(name=\"distance_score\")\n",
    "\n",
    "    mucompdata_pd = pd.merge(mucompdata_pd, cond_prob, on=\"cokey\", how=\"left\")\n",
    "    mucompdata_pd = mucompdata_pd.sort_values(\"distance_score\", ascending=False)\n",
    "    mucompdata_pd[\"distance_score_norm\"] = (\n",
    "        mucompdata_pd.distance_score / mucompdata_pd.distance_score.max()\n",
    "    ) * 0.25\n",
    "    mucompdata_pd = mucompdata_pd.reset_index(drop=True)\n",
    "    mucompdata_pd[\"distance\"] = mucompdata_pd[\"distance\"].round(4)\n",
    "    mucompdata_pd[\"Index\"] = mucompdata_pd.index\n",
    "\n",
    "    # Group by component name\n",
    "    mucompdata_grouped = mucompdata_pd.groupby(\"compname\", sort=False)\n",
    "\n",
    "    # Take at most 12 groups\n",
    "    mucompdata_comp_grps = [group for _, group in mucompdata_grouped][:12]\n",
    "\n",
    "    # Assign max distance scores to all members within each group\n",
    "    soilIDList_out = [assign_max_distance_scores(group) for group in mucompdata_comp_grps]\n",
    "\n",
    "    mucompdata_pd = pd.concat(soilIDList_out).reset_index(drop=True)\n",
    "    comp_key = mucompdata_pd[\"cokey\"].tolist()\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------------------\n",
    "    # Create horizon data table\n",
    "    columns_to_select = [\n",
    "        \"COMPID\",\n",
    "        \"TopDep\",\n",
    "        \"BotDep\",\n",
    "        \"id\",\n",
    "        \"Layer\",\n",
    "        \"SDTO\",\n",
    "        \"STPC\",\n",
    "        \"CLPC\",\n",
    "        \"CFRAG\",\n",
    "        \"CECS\",\n",
    "        \"PHAQ\",\n",
    "        \"ELCO\",\n",
    "        \"PROP\",\n",
    "        \"SU_name\",\n",
    "        \"FAO_SYS\",\n",
    "    ]\n",
    "    new_column_names = [\n",
    "        \"cokey\",\n",
    "        \"hzdept_r\",\n",
    "        \"hzdepb_r\",\n",
    "        \"chkey\",\n",
    "        \"hzname\",\n",
    "        \"sandtotal_r\",\n",
    "        \"silttotal_r\",\n",
    "        \"claytotal_r\",\n",
    "        \"total_frag_volume\",\n",
    "        \"CEC\",\n",
    "        \"pH\",\n",
    "        \"EC\",\n",
    "        \"comppct_r\",\n",
    "        \"compname\",\n",
    "        \"fss\",\n",
    "    ]\n",
    "\n",
    "    muhorzdata_pd = wise_data[columns_to_select]\n",
    "    muhorzdata_pd.columns = new_column_names\n",
    "    muhorzdata_pd = muhorzdata_pd[muhorzdata_pd[\"cokey\"].isin(comp_key)]\n",
    "    muhorzdata_pd[[\"hzdept_r\", \"hzdepb_r\"]] = (\n",
    "        muhorzdata_pd[[\"hzdept_r\", \"hzdepb_r\"]].fillna(0).astype(int)\n",
    "    )\n",
    "    muhorzdata_pd[\"texture\"] = muhorzdata_pd.apply(getTexture, axis=1)\n",
    "    muhorzdata_pd[\"texture\"] = muhorzdata_pd[\"texture\"].apply(\n",
    "        lambda x: str(x) if isinstance(x, np.ndarray) else x\n",
    "    )\n",
    "\n",
    "    # Rank components and sort by rank and depth\n",
    "    cokey_Index = {key: rank for rank, key in enumerate(comp_key)}\n",
    "    muhorzdata_pd[\"Comp_Rank\"] = muhorzdata_pd[\"cokey\"].map(cokey_Index)\n",
    "    muhorzdata_pd.sort_values([\"Comp_Rank\", \"hzdept_r\"], inplace=True)\n",
    "    muhorzdata_pd.drop(columns=\"Comp_Rank\", inplace=True)\n",
    "    muhorzdata_pd = muhorzdata_pd.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Check for duplicate component instances\n",
    "    hz_drop = drop_cokey_horz(muhorzdata_pd)\n",
    "    if hz_drop is not None:\n",
    "        muhorzdata_pd = muhorzdata_pd[~muhorzdata_pd.cokey.isin(hz_drop)]\n",
    "\n",
    "    # Update comp_key\n",
    "    comp_key = muhorzdata_pd[\"cokey\"].unique().tolist()\n",
    "\n",
    "    # Subset mucompdata_pd by new comp_key and add suffix to name if there are duplicates\n",
    "    mucompdata_pd = mucompdata_pd[mucompdata_pd[\"cokey\"].isin(comp_key)]\n",
    "    mucompdata_pd.sort_values([\"distance_score\", \"distance\"], ascending=[False, True], inplace=True)\n",
    "    mucompdata_pd.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Add suffix to duplicate names\n",
    "    name_counts = collections.Counter(mucompdata_pd[\"compname\"])\n",
    "    for name, count in name_counts.items():\n",
    "        if count > 1:\n",
    "            for suffix in range(1, count + 1):\n",
    "                mucompdata_pd.loc[mucompdata_pd[\"compname\"] == name, \"compname\"] = name + str(\n",
    "                    suffix\n",
    "                )\n",
    "\n",
    "    # Add modified compname to muhorzdata\n",
    "    muhorzdata_name = muhorzdata_pd[[\"cokey\"]].merge(\n",
    "        mucompdata_pd[[\"cokey\", \"compname\"]], on=\"cokey\"\n",
    "    )\n",
    "    muhorzdata_pd[\"compname\"] = muhorzdata_name[\"compname\"]\n",
    "\n",
    "    # Group data by cokey for texture\n",
    "    muhorzdata_group_cokey = list(muhorzdata_pd.groupby(\"cokey\", sort=False))\n",
    "\n",
    "    # Initialize lists for storing data\n",
    "    getProfile_cokey = []\n",
    "    c_bottom_depths = []\n",
    "    clay_texture = []\n",
    "    snd_lyrs = []\n",
    "    cly_lyrs = []\n",
    "    txt_lyrs = []\n",
    "    hz_lyrs = []\n",
    "    rf_lyrs = []\n",
    "    cec_lyrs = []\n",
    "    ph_lyrs = []\n",
    "    ec_lyrs = []\n",
    "\n",
    "    for group in muhorzdata_group_cokey:\n",
    "        for group_key, group in muhorzdata_group_cokey:  # Unpack the tuple\n",
    "            profile = (\n",
    "                group.sort_values(by=\"hzdept_r\")\n",
    "                .drop_duplicates(keep=\"first\")\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "        c_very_bottom = max_comp_depth(profile)\n",
    "\n",
    "        sand_pct_intpl = getProfile(profile, \"sandtotal_r\")\n",
    "        sand_pct_intpl.columns = [\"c_sandpct_intpl\", \"c_sandpct_intpl_grp\"]\n",
    "        clay_pct_intpl = getProfile(profile, \"claytotal_r\")\n",
    "        clay_pct_intpl.columns = [\"c_claypct_intpl\", \"c_claypct_intpl_grp\"]\n",
    "        cf_pct_intpl = getProfile(profile, \"total_frag_volume\")\n",
    "        cf_pct_intpl.columns = [\"c_cfpct_intpl\", \"c_cfpct_intpl_grp\"]\n",
    "        cec_intpl = getProfile(profile, \"CEC\")\n",
    "        cec_intpl.columns = [\"c_cec_intpl\"]\n",
    "        ph_intpl = getProfile(profile, \"pH\")\n",
    "        ph_intpl.columns = [\"c_ph_intpl\"]\n",
    "        ec_intpl = getProfile(profile, \"EC\")\n",
    "        ec_intpl.columns = [\"c_ec_intpl\"]\n",
    "\n",
    "        combined_data = pd.concat(\n",
    "            [\n",
    "                sand_pct_intpl[[\"c_sandpct_intpl_grp\"]],  # DataFrame\n",
    "                clay_pct_intpl[[\"c_claypct_intpl_grp\"]],  # DataFrame\n",
    "                cf_pct_intpl[[\"c_cfpct_intpl_grp\"]],     # DataFrame\n",
    "                pd.DataFrame(profile.compname.unique(), columns=[\"compname\"]),  # Convert to DataFrame\n",
    "                pd.DataFrame(profile.cokey.unique(), columns=[\"cokey\"]),        # Convert to DataFrame\n",
    "                pd.DataFrame(profile.comppct_r.unique(), columns=[\"comppct\"]),  # Convert to DataFrame\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        combined_data.columns = [\n",
    "            \"sandpct_intpl\",\n",
    "            \"claypct_intpl\",\n",
    "            \"rfv_intpl\",\n",
    "            \"compname\",\n",
    "            \"cokey\",\n",
    "            \"comppct\",\n",
    "        ]\n",
    "\n",
    "        c_bottom_temp = pd.DataFrame(\n",
    "            {\n",
    "                \"cokey\": [combined_data[\"cokey\"].iloc[0]],\n",
    "                \"compname\": [combined_data[\"compname\"].iloc[0]],\n",
    "                \"c_very_bottom\": [int(c_very_bottom)],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        snd_d, hz_depb = agg_data_layer(sand_pct_intpl.c_sandpct_intpl, bottom=c_bottom_temp[\"c_very_bottom\"].iloc[0], depth=True)\n",
    "        cly_d = agg_data_layer(clay_pct_intpl.c_claypct_intpl, bottom=c_bottom_temp[\"c_very_bottom\"].iloc[0], depth=False)\n",
    "        txt_d = [\n",
    "            getTexture(row=None, sand=s, silt=(100 - (s + c)), clay=c) for s, c in zip(snd_d, cly_d)\n",
    "        ]\n",
    "        txt_d = pd.Series(txt_d, index=snd_d.index)\n",
    "\n",
    "        rf_d = agg_data_layer(cf_pct_intpl.c_cfpct_intpl, bottom=c_bottom_temp[\"c_very_bottom\"].iloc[0], depth=False)\n",
    "        cec_d = agg_data_layer(cec_intpl.c_cec_intpl, bottom=c_bottom_temp[\"c_very_bottom\"].iloc[0], depth=False)\n",
    "        ph_d = agg_data_layer(ph_intpl.c_ph_intpl, bottom=c_bottom_temp[\"c_very_bottom\"].iloc[0], depth=False)\n",
    "        ec_d = agg_data_layer(ec_intpl.c_ec_intpl, bottom=c_bottom_temp[\"c_very_bottom\"].iloc[0], depth=False)\n",
    "\n",
    "        # Fill NaN values and append to lists\n",
    "        for data_list, data in zip(\n",
    "            [snd_lyrs, cly_lyrs, txt_lyrs, rf_lyrs, cec_lyrs, ph_lyrs, ec_lyrs, hz_lyrs],\n",
    "            [snd_d, cly_d, txt_d, rf_d, cec_d, ph_d, ec_d, hz_depb],\n",
    "        ):\n",
    "            data_list.append(dict(data.fillna(\"\")))\n",
    "\n",
    "        c_bottom_depths.append(c_bottom_temp)\n",
    "        getProfile_cokey.append(combined_data)\n",
    "\n",
    "        comp_texture_list = [x for x in profile.texture.str.lower() if x]\n",
    "        clay_val = \"Yes\" if any(\"clay\" in string for string in comp_texture_list) else \"No\"\n",
    "        clay_texture_temp = pd.DataFrame(\n",
    "            {\"compname\": [combined_data[\"compname\"].iloc[0]], \"clay\": [clay_val]}\n",
    "        )\n",
    "        clay_texture.append(clay_texture_temp)\n",
    "\n",
    "    # Concatenate lists to form DataFrames\n",
    "    c_bottom_depths = pd.concat(c_bottom_depths, axis=0)\n",
    "    clay_texture = pd.concat(clay_texture, axis=0)\n",
    "\n",
    "    # Subset mucompdata and muhorzdata DataFrames\n",
    "    mucompdata_pd = mucompdata_pd[mucompdata_pd[\"cokey\"].isin(c_bottom_depths.cokey)]\n",
    "    muhorzdata_pd = muhorzdata_pd[muhorzdata_pd[\"cokey\"].isin(c_bottom_depths.cokey)]\n",
    "\n",
    "    # Merge c_bottom_depth and clay_texture with mucompdata\n",
    "    mucompdata_pd = pd.merge(\n",
    "        mucompdata_pd, c_bottom_depths[[\"compname\", \"c_very_bottom\"]], on=\"compname\", how=\"left\"\n",
    "    )\n",
    "    mucompdata_pd = pd.merge(mucompdata_pd, clay_texture, on=\"compname\", how=\"left\")\n",
    "\n",
    "    # Create index for component instance display\n",
    "    mucompdata_comp_grps_list = []\n",
    "    mucompdata_comp_grps = [group for _, group in mucompdata_pd.groupby(\"compname_grp\", sort=False)]\n",
    "\n",
    "    for group in mucompdata_comp_grps:\n",
    "        group = group.sort_values(\"distance\").reset_index(drop=True)\n",
    "        soilID_rank = [True if idx == 0 else False for idx in range(len(group))]\n",
    "\n",
    "        group[\"soilID_rank\"] = soilID_rank\n",
    "        group[\"min_dist\"] = group.distance.iloc[0]\n",
    "\n",
    "        mucompdata_comp_grps_list.append(group)\n",
    "\n",
    "    mucompdata_pd = pd.concat(mucompdata_comp_grps_list).reset_index(drop=True)\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------------------------------\n",
    "    # SoilIDList output\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Format output data\n",
    "    soilIDRank_output = [\n",
    "        group[[\"compname\", \"sandpct_intpl\", \"claypct_intpl\", \"rfv_intpl\"]]\n",
    "        for group in getProfile_cokey\n",
    "    ]\n",
    "    soilIDRank_output_pd = pd.concat(soilIDRank_output, axis=0).reset_index(drop=True)\n",
    "\n",
    "    mucompdata_cond_prob = mucompdata_pd.sort_values(\n",
    "        \"distance_score_norm\", ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Determine rank location\n",
    "    rank_id = 1\n",
    "    Rank_Loc = []\n",
    "    for rank in mucompdata_cond_prob[\"soilID_rank\"]:\n",
    "        Rank_Loc.append(str(rank_id) if rank else \"Not Displayed\")\n",
    "        rank_id += rank  # Increase rank_id only if rank is True\n",
    "    mucompdata_cond_prob[\"Rank_Loc\"] = Rank_Loc\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------------------------------\n",
    "    # API output: Code outputs HWSD data without any alteration (i.e., texture class averaging)\n",
    "\n",
    "    # Handle NaN values\n",
    "    mucompdata_cond_prob.replace([np.nan, \"nan\", \"None\", [None]], \"\", inplace=True)\n",
    "\n",
    "    mucomp_index = mucompdata_cond_prob.sort_values(\n",
    "        [\"soilID_rank\", \"distance_score_norm\"], ascending=[False, False], inplace=True\n",
    "    )\n",
    "\n",
    "    # Extract lists for constructing ID dictionary\n",
    "    siteName = mucompdata_cond_prob[\"compname\"].apply(lambda x: x.capitalize()).tolist()\n",
    "    compName = mucompdata_cond_prob[\"compname_grp\"].apply(lambda x: x.capitalize()).tolist()\n",
    "    score = mucompdata_cond_prob[\"distance_score_norm\"].round(3).tolist()\n",
    "    rank_loc = mucompdata_cond_prob[\"Rank_Loc\"].tolist()\n",
    "    model_version = 3\n",
    "\n",
    "    # Step 3: Construct ID list directly from the sorted and cleaned DataFrame\n",
    "    ID = []\n",
    "    for i, idx in enumerate(mucompdata_cond_prob.index):\n",
    "        idT = {\n",
    "            \"name\": siteName[i],\n",
    "            \"component\": compName[i],\n",
    "            \"score_loc\": score[i],\n",
    "            \"rank_loc\": rank_loc[i],\n",
    "        }\n",
    "        ID.append(idT)\n",
    "\n",
    "    # Merge component descriptions\n",
    "    WRB_Comp_Desc = get_WRB_descriptions(\n",
    "        mucompdata_cond_prob[\"compname_grp\"].drop_duplicates().tolist()\n",
    "    )\n",
    "    mucompdata_cond_prob = pd.merge(\n",
    "        mucompdata_cond_prob, WRB_Comp_Desc, left_on=\"compname_grp\", right_on=\"WRB_tax\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Extract site information\n",
    "    Site = [\n",
    "        {\n",
    "            \"siteData\": {\n",
    "                \"mapunitID\": row.mukey,\n",
    "                \"componentID\": row.cokey,\n",
    "                \"fao\": row.fss,\n",
    "                \"share\": row.share,\n",
    "                \"distance\": round(row.distance, 3),\n",
    "                \"minCompDistance\": row.min_dist,\n",
    "                \"soilDepth\": row.c_very_bottom,\n",
    "            },\n",
    "            \"siteDescription\": {\n",
    "                key: row[key]\n",
    "                for key in [\n",
    "                    \"Description_en\",\n",
    "                    \"Management_en\",\n",
    "                    \"Description_es\",\n",
    "                    \"Management_es\",\n",
    "                    \"Description_ks\",\n",
    "                    \"Management_ks\",\n",
    "                    \"Description_fr\",\n",
    "                    \"Management_fr\",\n",
    "                ]\n",
    "            },\n",
    "        }\n",
    "        for _, row in mucompdata_cond_prob.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Reorder lists based on mucomp_index\n",
    "    hz_lyrs = [hz_lyrs[i] for i in mucomp_index]\n",
    "    snd_lyrs = [snd_lyrs[i] for i in mucomp_index]\n",
    "    cly_lyrs = [cly_lyrs[i] for i in mucomp_index]\n",
    "    txt_lyrs = [txt_lyrs[i] for i in mucomp_index]\n",
    "    rf_lyrs = [rf_lyrs[i] for i in mucomp_index]\n",
    "    cec_lyrs = [cec_lyrs[i] for i in mucomp_index]\n",
    "    ph_lyrs = [ph_lyrs[i] for i in mucomp_index]\n",
    "    ec_lyrs = [ec_lyrs[i] for i in mucomp_index]\n",
    "\n",
    "    output_SoilList = [\n",
    "        dict(\n",
    "            zip(\n",
    "                [\n",
    "                    \"id\",\n",
    "                    \"site\",\n",
    "                    \"bottom_depth\",\n",
    "                    \"sand\",\n",
    "                    \"clay\",\n",
    "                    \"texture\",\n",
    "                    \"rock_fragments\",\n",
    "                    \"cec\",\n",
    "                    \"ph\",\n",
    "                    \"ec\",\n",
    "                ],\n",
    "                item,\n",
    "            )\n",
    "        )\n",
    "        for item in zip(\n",
    "            ID, Site, hz_lyrs, snd_lyrs, cly_lyrs, txt_lyrs, rf_lyrs, cec_lyrs, ph_lyrs, ec_lyrs\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Save data\n",
    "    if plot_id is None:\n",
    "        soilIDRank_output_pd.to_csv(config.SOIL_ID_RANK_PATH, index=None, header=True)\n",
    "        mucompdata_cond_prob.to_csv(config.SOIL_ID_PROB_PATH, index=None, header=True)\n",
    "    else:\n",
    "        save_model_output(\n",
    "            plot_id,\n",
    "            model_version,\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"metadata\": {\n",
    "                        \"location\": \"global\",\n",
    "                        \"model\": \"v2\",\n",
    "                        \"unit_measure\": {\n",
    "                            \"distance\": \"m\",\n",
    "                            \"depth\": \"cm\",\n",
    "                            \"cec\": \"cmol(c)/kg\",\n",
    "                            \"clay\": \"%\",\n",
    "                            \"rock_fragments\": \"cm3/100cm3\",\n",
    "                            \"sand\": \"%\",\n",
    "                            \"ec\": \"ds/m\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"soilList\": output_SoilList,\n",
    "                }\n",
    "            ),\n",
    "            soilIDRank_output_pd.to_csv(index=None, header=True),\n",
    "            mucompdata_cond_prob.to_csv(index=None, header=True),\n",
    "        )\n",
    "\n",
    "    # Return the JSON output\n",
    "    return {\n",
    "        \"metadata\": {\n",
    "            \"location\": \"global\",\n",
    "            \"model\": \"v2\",\n",
    "            \"unit_measure\": {\n",
    "                \"distance\": \"m\",\n",
    "                \"depth\": \"cm\",\n",
    "                \"cec\": \"cmol(c)/kg\",\n",
    "                \"clay\": \"%\",\n",
    "                \"rock_fragments\": \"cm3/100cm3\",\n",
    "                \"sand\": \"%\",\n",
    "                \"ec\": \"ds/m\",\n",
    "            },\n",
    "        },\n",
    "        \"soilList\": output_SoilList,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ad57ba-2aca-4e12-98aa-5dc8ff3c64d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25371/1466463946.py:98: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = geometry.centroid.iloc[0]\n",
      "/tmp/ipykernel_25371/1466463946.py:98: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = geometry.centroid.iloc[0]\n",
      "/tmp/ipykernel_25371/1466463946.py:220: FutureWarning: The provided callable <built-in function min> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  mu_id_dist[\"distance\"] = mu_id_dist.groupby(\"MUGLB_NEW\")[\"dist_meters\"].transform(min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using UTM CRS: EPSG:32630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25371/2886632837.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mucompdata_pd[\"distance\"] = pd.to_numeric(mucompdata_pd[\"distance\"])\n",
      "/tmp/ipykernel_25371/2886632837.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mucompdata_pd[\"share\"] = pd.to_numeric(mucompdata_pd[\"share\"])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'compname_grp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m global_return \u001b[38;5;241m=\u001b[39m \u001b[43mgetSoilLocationBasedGlobal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.57\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(global_return)\n",
      "Cell \u001b[0;32mIn[6], line 263\u001b[0m, in \u001b[0;36mgetSoilLocationBasedGlobal\u001b[0;34m(lon, lat, plot_id)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Create index for component instance display\u001b[39;00m\n\u001b[1;32m    262\u001b[0m mucompdata_comp_grps_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 263\u001b[0m mucompdata_comp_grps \u001b[38;5;241m=\u001b[39m [group \u001b[38;5;28;01mfor\u001b[39;00m _, group \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmucompdata_pd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompname_grp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m]\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m mucompdata_comp_grps:\n\u001b[1;32m    266\u001b[0m     group \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/c/Users/jmaynard/Documents/GitHub/soil-id-algorithm/env/lib/python3.12/site-packages/pandas/core/frame.py:9170\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   9168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 9170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9173\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9176\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/jmaynard/Documents/GitHub/soil-id-algorithm/env/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/mnt/c/Users/jmaynard/Documents/GitHub/soil-id-algorithm/env/lib/python3.12/site-packages/pandas/core/groupby/grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'compname_grp'"
     ]
    }
   ],
   "source": [
    "global_return = getSoilLocationBasedGlobal(lon = -1.57, lat=8.95, plot_id=1)\n",
    "print(global_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eb3ed88-099b-476b-bc70-bfe9e90d1450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'pandas.core.series.Series'>, (7,))\n"
     ]
    }
   ],
   "source": [
    "print(global_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4cac27-06a2-4527-8e8f-1ad908d9e6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
